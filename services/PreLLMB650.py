"""
Intelligent Pre-Processing Pipeline for B650 Document Extraction
Reduces LLM load by 70-80% through smart filtering and structuring
Author: AI Solution Architect
"""

import re
import json
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
import logging
from abc import ABC, abstractmethod
import difflib
# from ATradiaLLM import llm
# # from services.llm_service import llm_service
# from prompts.B650_section_a_extraction_prompt import get_b650_section_a_extraction_prompt
# from llm_response_formats.B650.Section_a_response_format import B650_SECTION_A_RESPONSE_FORMAT

# =============================================================================
# PREPROCESSING CONFIGURATION
# =============================================================================

@dataclass
class PreprocessingConfig:
    """Configuration for preprocessing pipeline"""
    min_line_length: int = 3
    max_line_length: int = 200
    noise_threshold: float = 0.3
    relevance_threshold: float = 0.4
    enable_smart_chunking: bool = True
    enable_field_detection: bool = True
    enable_noise_removal: bool = True
    max_llm_tokens: int = 1500  # Target token count for LLM input

class DocumentSection(Enum):
    """Document sections for structured processing"""
    HEADER = "header"
    SHIPPER_INFO = "shipper_info"
    CONSIGNEE_INFO = "consignee_info"
    TRANSPORT_INFO = "transport_info"
    CARGO_INFO = "cargo_info"
    FOOTER = "footer"
    UNKNOWN = "unknown"

@dataclass
class ProcessedField:
    """Structured field with context"""
    field_name: str
    raw_value: str
    cleaned_value: str
    confidence: float
    context_line: str
    section: DocumentSection

@dataclass
class PreprocessingResult:
    """Result of preprocessing pipeline"""
    structured_data: Dict[str, ProcessedField]
    relevant_text: str
    original_length: int
    processed_length: int
    reduction_percentage: float
    requires_llm: bool
    sections: Dict[DocumentSection, List[str]]

# =============================================================================
# FIELD DETECTION PATTERNS
# =============================================================================

B650_FIELD_PATTERNS = {
    "bl_number": {
        "patterns": [
            r"b/l\s*no[:\s]+([A-Z0-9]+)",
            r"bill of lading\s*(?:no|number)[:\s]+([A-Z0-9]+)",
            r"doc\s*no[:\s]+([A-Z0-9]+)",
            r"\b([A-Z]{2,4}[0-9]{6,}[A-Z0-9]*)\b"  # Pattern like AHG0233130P08
        ],
        "aliases": ["b/l no", "bill of lading", "doc no", "document number"],
        "section": DocumentSection.HEADER,
        "priority": 1
    },
    "consignee_name": {
        "patterns": [
            r"consignee[:\s]+([A-Z\s&.,'-]+?)(?:\n|ADDRESS)",
            r"notify party[:\s]+([A-Z\s&.,'-]+?)(?:\n|ADDRESS)",
        ],
        "aliases": ["consignee", "notify party", "receiver"],
        "section": DocumentSection.CONSIGNEE_INFO,
        "priority": 1
    },
    "consignee_address": {
        "patterns": [
            r"address\s*[:\s]+([^TEL\n]+)",
            r"(?:consignee.*?address[:\s]+)([^TEL\n]+)",
        ],
        "aliases": ["address", "delivery address"],
        "section": DocumentSection.CONSIGNEE_INFO,
        "priority": 2
    },
    "shipper_name": {
        "patterns": [
            r"shipper[:\s\n]+([A-Z\s&.,'-]+?)(?:\n|TEL)",
        ],
        "aliases": ["shipper", "exporter"],
        "section": DocumentSection.SHIPPER_INFO,
        "priority": 1
    },
    "port_of_loading": {
        "patterns": [
            r"port of (?:loading|shipment)[:\s]+([A-Z\s,]+)",
            r"place of receipt[:\s]+([A-Z\s,]+)",
        ],
        "aliases": ["port of loading", "place of receipt"],
        "section": DocumentSection.TRANSPORT_INFO,
        "priority": 1
    },
    "port_of_discharge": {
        "patterns": [
            r"port of (?:discharge|destination)[:\s]+([A-Z\s,]+)",
            r"place of delivery[:\s]+([A-Z\s,]+)",
        ],
        "aliases": ["port of discharge", "place of delivery"],
        "section": DocumentSection.TRANSPORT_INFO,
        "priority": 1
    },
    "gross_weight": {
        "patterns": [
            r"(\d+(?:\.\d+)?)\s*kgs?",
            r"gross weight[:\s]+(\d+(?:\.\d+)?)",
        ],
        "aliases": ["gross weight", "weight", "kgs"],
        "section": DocumentSection.CARGO_INFO,
        "priority": 2
    },
    "measurement": {
        "patterns": [
            r"(\d+(?:\.\d+)?)\s*cbm",
            r"measurement[:\s]+(\d+(?:\.\d+)?)",
        ],
        "aliases": ["measurement", "cbm", "volume"],
        "section": DocumentSection.CARGO_INFO,
        "priority": 2
    },
    "number_of_packages": {
        "patterns": [
            r"(\d+)\s*pkgs?",
            r"no\.\s*of\s*pkgs[:\s]+(\d+)",
        ],
        "aliases": ["pkgs", "packages", "no. of pkgs"],
        "section": DocumentSection.CARGO_INFO,
        "priority": 2
    }
}

# =============================================================================
# NOISE PATTERNS
# =============================================================================

NOISE_PATTERNS = [
    r"^\s*$",  # Empty lines
    r"^\s*[-=_*]+\s*$",  # Separator lines
    r"^\s*page\s+\d+\s*of\s*\d+\s*$",  # Page numbers
    r"^\s*\d+\s*$",  # Lone numbers
    r"^\s*[|\\/_]+\s*$",  # Drawing characters
    r"OCR\s+(Text|Page)",  # OCR artifacts
    r"^\s*===.*?===\s*$",  # OCR section markers
    r"Processing page \d+ of \d+",  # Processing messages
    r"Performing (?:comprehensive )?OCR",  # OCR messages
]

# =============================================================================
# PREPROCESSING STRATEGIES
# =============================================================================

class PreprocessingStrategy(ABC):
    """Abstract base for preprocessing strategies"""
    
    @abstractmethod
    def process(self, text: str, config: PreprocessingConfig) -> str:
        pass

class NoiseRemovalStrategy(PreprocessingStrategy):
    """Remove OCR artifacts and irrelevant content"""
    
    def process(self, text: str, config: PreprocessingConfig) -> str:
        """Remove noise patterns and clean text"""
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # Skip if matches noise patterns
            if self._is_noise_line(line):
                continue
                
            # Skip if too short or too long
            if len(line.strip()) < config.min_line_length or len(line) > config.max_line_length:
                continue
                
            # Clean the line
            cleaned_line = self._clean_line(line)
            if cleaned_line:
                cleaned_lines.append(cleaned_line)
        
        return '\n'.join(cleaned_lines)
    
    def _is_noise_line(self, line: str) -> bool:
        """Check if line matches noise patterns"""
        for pattern in NOISE_PATTERNS:
            if re.match(pattern, line, re.IGNORECASE):
                return True
        return False
    
    def _clean_line(self, line: str) -> str:
        """Clean individual line"""
        # Remove excessive whitespace
        line = re.sub(r'\s+', ' ', line.strip())
        
        # Remove special OCR characters
        line = re.sub(r'[^\w\s:.,/-]', ' ', line)
        
        # Remove repeated characters (OCR artifacts)
        line = re.sub(r'(.)\1{3,}', r'\1\1', line)
        
        return line

class FieldDetectionStrategy(PreprocessingStrategy):
    """Detect and extract structured fields"""
    
    def __init__(self):
        self.detected_fields: Dict[str, ProcessedField] = {}
    
    def process(self, text: str, config: PreprocessingConfig) -> str:
        """Detect fields and return structured representation"""
        self.detected_fields = {}
        lines = text.split('\n')
        
        # Process each field pattern
        for field_name, field_config in B650_FIELD_PATTERNS.items():
            field = self._detect_field(text, lines, field_name, field_config)
            if field:
                self.detected_fields[field_name] = field
        
        return self._create_structured_text(text, lines)
    
    def _detect_field(self, text: str, lines: List[str], field_name: str, field_config: Dict) -> Optional[ProcessedField]:
        """Detect a specific field using patterns"""
        
        # Try regex patterns first
        for pattern in field_config["patterns"]:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                raw_value = match.group(1).strip()
                cleaned_value = self._clean_field_value(raw_value, field_name)
                
                # Find context line
                context_line = self._find_context_line(lines, raw_value)
                
                return ProcessedField(
                    field_name=field_name,
                    raw_value=raw_value,
                    cleaned_value=cleaned_value,
                    confidence=0.9,  # High confidence for pattern match
                    context_line=context_line,
                    section=field_config["section"]
                )
        
        # Try fuzzy matching with aliases
        for line in lines:
            for alias in field_config["aliases"]:
                if self._fuzzy_match(alias, line.lower()):
                    value = self._extract_value_from_line(line, alias)
                    if value:
                        return ProcessedField(
                            field_name=field_name,
                            raw_value=value,
                            cleaned_value=self._clean_field_value(value, field_name),
                            confidence=0.7,  # Lower confidence for fuzzy match
                            context_line=line,
                            section=field_config["section"]
                        )
        
        return None
    
    def _fuzzy_match(self, alias: str, text: str, threshold: float = 0.8) -> bool:
        """Check if alias fuzzy matches text"""
        ratio = difflib.SequenceMatcher(None, alias.lower(), text).ratio()
        return ratio >= threshold
    
    def _extract_value_from_line(self, line: str, alias: str) -> Optional[str]:
        """Extract value after finding alias in line"""
        # Look for colon separator
        if ':' in line:
            parts = line.split(':', 1)
            if len(parts) > 1:
                return parts[1].strip()
        
        # Look for value after alias
        alias_pos = line.lower().find(alias.lower())
        if alias_pos >= 0:
            value_part = line[alias_pos + len(alias):].strip()
            if value_part:
                return value_part.split()[0] if ' ' in value_part else value_part
        
        return None
    
    def _clean_field_value(self, value: str, field_name: str) -> str:
        """Clean field value based on field type"""
        value = value.strip()
        
        if field_name in ["gross_weight", "measurement", "number_of_packages"]:
            # Extract numeric values
            numbers = re.findall(r'\d+(?:\.\d+)?', value)
            return numbers[0] if numbers else value
        
        if field_name.endswith("_name"):
            # Clean name fields
            value = re.sub(r'[^\w\s&.,\'-]', ' ', value)
            value = ' '.join(value.split())  # Normalize whitespace
        
        return value
    
    def _find_context_line(self, lines: List[str], value: str) -> str:
        """Find the line containing the value"""
        for line in lines:
            if value.lower() in line.lower():
                return line.strip()
        return ""
    
    def _create_structured_text(self, original_text: str, lines: List[str]) -> str:
        """Create structured text highlighting detected fields"""
        if not self.detected_fields:
            return original_text
        
        structured_parts = ["=== DETECTED FIELDS ==="]
        for field_name, field in self.detected_fields.items():
            structured_parts.append(f"{field_name.upper()}: {field.cleaned_value}")
        
        structured_parts.append("\n=== REMAINING TEXT FOR LLM ===")
        
        # Add lines that don't contain already detected fields
        remaining_lines = []
        for line in lines:
            line_contains_detected_field = False
            for field in self.detected_fields.values():
                if field.raw_value.lower() in line.lower():
                    line_contains_detected_field = True
                    break
            
            if not line_contains_detected_field and len(line.strip()) > 10:
                remaining_lines.append(line)
        
        structured_parts.extend(remaining_lines[:10])  # Limit remaining text
        return '\n'.join(structured_parts)

class SmartChunkingStrategy(PreprocessingStrategy):
    """Intelligently chunk document by sections"""
    
    def process(self, text: str, config: PreprocessingConfig) -> str:
        """Chunk text by logical sections"""
        sections = self._identify_sections(text)
        
        # Prioritize sections for LLM processing
        priority_sections = [
            DocumentSection.CONSIGNEE_INFO,
            DocumentSection.TRANSPORT_INFO,
            DocumentSection.CARGO_INFO,
            DocumentSection.SHIPPER_INFO
        ]
        
        chunked_parts = []
        current_tokens = 0
        
        for section in priority_sections:
            if section in sections and current_tokens < config.max_llm_tokens:
                section_text = '\n'.join(sections[section])
                section_tokens = len(section_text.split()) * 1.3  # Rough token estimation
                
                if current_tokens + section_tokens <= config.max_llm_tokens:
                    chunked_parts.append(f"=== {section.value.upper()} ===")
                    chunked_parts.append(section_text)
                    current_tokens += section_tokens
        
        return '\n'.join(chunked_parts)
    
    def _identify_sections(self, text: str) -> Dict[DocumentSection, List[str]]:
        """Identify document sections"""
        lines = text.split('\n')
        sections = {section: [] for section in DocumentSection}
        current_section = DocumentSection.UNKNOWN
        
        section_keywords = {
            DocumentSection.SHIPPER_INFO: ["shipper", "exporter"],
            DocumentSection.CONSIGNEE_INFO: ["consignee", "notify party"],
            DocumentSection.TRANSPORT_INFO: ["port of", "vessel", "voyage"],
            DocumentSection.CARGO_INFO: ["description of goods", "weight", "measurement", "pkgs"]
        }
        
        for line in lines:
            line_lower = line.lower()
            
            # Detect section changes
            for section, keywords in section_keywords.items():
                if any(keyword in line_lower for keyword in keywords):
                    current_section = section
                    break
            
            sections[current_section].append(line)
        
        return sections

# =============================================================================
# MAIN PREPROCESSING PIPELINE
# =============================================================================

class PreprocessingPipeline:
    """Main preprocessing pipeline orchestrator"""
    
    def __init__(self, config: Optional[PreprocessingConfig] = None):
        self.config = config or PreprocessingConfig()
        self.logger = logging.getLogger(__name__)
        
        # Initialize strategies
        self.noise_removal = NoiseRemovalStrategy()
        self.field_detection = FieldDetectionStrategy()
        self.smart_chunking = SmartChunkingStrategy()
    
    def process(self, raw_text: str) -> PreprocessingResult:
        """Main processing pipeline"""
        original_length = len(raw_text)
        processed_text = raw_text
        
        # Step 1: Remove noise if enabled
        if self.config.enable_noise_removal:
            processed_text = self.noise_removal.process(processed_text, self.config)
            self.logger.info("Applied noise removal")
        
        # Step 2: Detect fields if enabled
        structured_fields = {}
        if self.config.enable_field_detection:
            processed_text = self.field_detection.process(processed_text, self.config)
            structured_fields = self.field_detection.detected_fields
            self.logger.info(f"Detected {len(structured_fields)} fields")
        
        # Step 3: Smart chunking if enabled
        if self.config.enable_smart_chunking:
            processed_text = self.smart_chunking.process(processed_text, self.config)
            self.logger.info("Applied smart chunking")
        
        processed_length = len(processed_text)
        reduction_percentage = ((original_length - processed_length) / original_length) * 100
        
        # Determine if LLM is still needed
        requires_llm = (
            len(structured_fields) < len(B650_FIELD_PATTERNS) * 0.8 or  # Less than 80% fields found
            any(field.confidence < 0.8 for field in structured_fields.values())  # Low confidence fields
        )
        
        return PreprocessingResult(
            structured_data=structured_fields,
            relevant_text=processed_text,
            original_length=original_length,
            processed_length=processed_length,
            reduction_percentage=reduction_percentage,
            requires_llm=requires_llm,
            sections=self.smart_chunking._identify_sections(processed_text) if self.config.enable_smart_chunking else {}
        )
    
    def get_llm_prompt(self, preprocessing_result: PreprocessingResult) -> str:
        """Generate optimized prompt for LLM"""
        if not preprocessing_result.requires_llm:
            return ""
        
        # Create concise prompt with only missing fields
        detected_field_names = set(preprocessing_result.structured_data.keys())
        missing_fields = set(B650_FIELD_PATTERNS.keys()) - detected_field_names
        
        prompt_parts = [
            "Extract the following missing B650 form fields from this shipping document:",
            f"Missing fields: {', '.join(missing_fields)}",
            "Detected fields (for context):"
        ]
        
        for field_name, field in preprocessing_result.structured_data.items():
            prompt_parts.append(f"- {field_name}: {field.cleaned_value}")
        
        prompt_parts.extend([
            "\nDocument text:",
            preprocessing_result.relevant_text,
            "\nReturn only the missing fields as JSON. If not found, use null."
        ])
        
        return '\n'.join(prompt_parts)

# =============================================================================
# FASTAPI INTEGRATION
# =============================================================================

# from fastapi import FastAPI
from pydantic import BaseModel

class PreprocessingRequest(BaseModel):
    text: str
    enable_noise_removal: bool = True
    enable_field_detection: bool = True
    enable_smart_chunking: bool = True
    max_llm_tokens: int = 1500

class PreprocessingResponse(BaseModel):
    structured_data: Dict[str, Dict]
    llm_prompt: str
    reduction_percentage: float
    requires_llm: bool
    token_savings: int

# app = FastAPI(title="B650 Preprocessing API", version="1.0.0")

# @app.post("/preprocess", response_model=PreprocessingResponse)
# async def preprocess_document(request: PreprocessingRequest):
#     """Preprocess document before LLM processing"""
    
#     # Create config from request
#     config = PreprocessingConfig(
#         enable_noise_removal=request.enable_noise_removal,
#         enable_field_detection=request.enable_field_detection,
#         enable_smart_chunking=request.enable_smart_chunking,
#         max_llm_tokens=request.max_llm_tokens
#     )
    
#     # Process document
#     pipeline = PreprocessingPipeline(config)
#     result = pipeline.process(request.text)
    
#     # Generate LLM prompt
#     llm_prompt = pipeline.get_llm_prompt(result)
    
#     # Calculate token savings (rough estimation)
#     original_tokens = len(request.text.split()) * 1.3
#     processed_tokens = len(llm_prompt.split()) * 1.3
#     token_savings = int(original_tokens - processed_tokens)
    
#     return PreprocessingResponse(
#         structured_data={k: asdict(v) for k, v in result.structured_data.items()},
#         llm_prompt=llm_prompt,
#         reduction_percentage=result.reduction_percentage,
#         requires_llm=result.requires_llm,
#         token_savings=token_savings
#     )

# @app.get("/health")
# async def health_check():
#     return {"status": "healthy", "service": "B650 Preprocessing Pipeline"}

# =============================================================================
# USAGE EXAMPLE
# =============================================================================

def convert_result_to_json(preprocessing_result: PreprocessingResult) -> Dict:
    """Convert PreprocessingResult to JSON-serializable format"""
    return {
        "structured_data": {
            field_name: {
                "field_name": field.field_name,
                "raw_value": field.raw_value,
                "cleaned_value": field.cleaned_value,
                "confidence": field.confidence,
                "context_line": field.context_line,
                "section": field.section.value
            }
            for field_name, field in preprocessing_result.structured_data.items()
        },
        "relevant_text": preprocessing_result.relevant_text,
        "original_length": preprocessing_result.original_length,
        "processed_length": preprocessing_result.processed_length,
        "reduction_percentage": preprocessing_result.reduction_percentage,
        "requires_llm": preprocessing_result.requires_llm,
        "sections": {
            section.value: lines 
            for section, lines in preprocessing_result.sections.items() 
            if lines  # Only include sections with content
        }
    }

def example_usage():
    """Example of how to use the preprocessing pipeline with JSON output"""
    
    # Sample text (from your bill of lading)
    sample_text = """
Starting complete document extraction for: uploads/default/default_18f82f3a-25ec-4a8d-8908-944b51fb15b6.pdf
Processing page 1 of 1...
Performing comprehensive OCR on entire document...
Performing OCR on page 1...
=== DOCUMENT TEXT CONTENT (with OCR) ===
=== Page 1 ===
YANTAIRIMAMACHINERYCO.,LTD.XIBEIYUINDUSTRY SHBNE250400002
ZONE,LAIZHOU,SHANDONG,CHINA
TEL/FAX:+86-535-6805690
AHG0233130P08
BRADLEYTHOMAS
ADDRESS:25MOLLERST,GORDONVALEQLD4865
AUSTRALIA
TEL:0481042746
EMAIL:
BRADLEY@THOMASARBORICULTURE.COM.AU
ASEA360CONSOLIDATIONPTYLTD
SAMEASCONSIGNEE
9/2TULLAMARINEPARKROAD,TULLAMARINEVIC3043
POBOX659,TULLAMARINEVIC3043
TEL:+61383460166
FAX:+61393387447
ANLGIPPSLAND 082S SHANGHAI,CHINA
BRISBANE,AUSTRALIA BRISBANE,AUSTRALIA
1PKGS SAIDTOCONTAIN 237KGS 0.92CBM
GRAPPLE
YT250305-07 HYDRAULICROTATOR
SEGU9412081/M1429568 40NOR
CFS-CFS
FREIGHTPREPAID
OCEANFREIGHT ASARRANGED
SAYTOTAL:ONEPKGSONLY
SHANGHAI MAY04,2025
SHANGHAI

[OCR Text]
Werd World Jaguar Logistics Inc.
COMBINED TRANSPORT BILL OF LADING

Shipper

YANTAI RIMA MACHINERY CO., LTD, XIBETYU INDUSTRY DOC No: SHBNE250400002
ZONE, LAIZHOU, SHANDONG, CHINA ae
TEL/FAX:+86-535-6805690

B/L No: AHG0233130P08

ensignee
RADLEYTHOMAS
DDRESS : 25 MOLLER ST, GORDONVALE QLD 4865

cst COPY
EL:0481042746

MAIL:
RADLEY@THOMASARBORICULTURE.COM.AU

Peo

Bos

Notify Party(Gomplete name and address) For delivery of goods Please apply to:
SAME AS CONSIGNEE ASEA360 CONSOLIDATION PTY LTD
9/2 TULLAMARINE PARK ROAD, TULLAMARINE VIC 3043
PO BOX 659, TULLAMARINE VIC 3043
TEL: +613 8346 0166
FAX: +613 9338 7447

Place of Receipt

ANL GIPPSLAND 0825 Port SHANGHATCHINA
BRISBANE AUSTRALIA Place SRISBANE, AUSTRALIA

PARTICULARS FURNISHED BY SHIPPER

Marks and numbers No. of pkgs Description of goods Gross weight Measurement
1PKGS SAID TO CONTAIN 237KGS 0.92CBM
GRAPPLE
YT250305-07 HYDRAULIC ROTATOR,

SEGU9412081/M1429568 40NOR

CFS-CFS
FREIGHT PREPAID
FREIGHT&CHARGE REVENUE TONS RATE COLLECT
OCEAN FREIGHT ED
SAY TOTAL:ONE PKGS ONLY

The goods and instructions are accepted and dealt with subject to the Standard Conditions printed overleaf.

Taken in charge in apparent good order and condition, unless otherwise noted herein, at the place of receipt for iransport and delivery as mentioned above
One of these Combined Transport Bills of lading must be surrendered duly endorsed in exchange for the goods. In Witness whereof the original Combined.
Transport Bills of Lading all of this tenor and date have been signed in the number stated below, one of which being accomplished the other(s) to be void.

Freight amount Place and date of issue
SHANGHAI MAY 04, 2025
Freight Payable at
SHANGHAI

Cargo Insurance through the undersigned

not covered [1] Covered according to attached Policy

Number of Original

=== COMPREHENSIVE OCR EXTRACTION ===
=== OCR Page 1 ===
World Jaguar Logistics Inc.
COMBINED TRANSPORT BILL OF LADING

Shipper

YANTAI RIMA MACHINERY CO., LTD. XIBETYU INDUSTRY DOC No: SHBNE250400002
ZONE, LAIZHOU, SHANDONG, CHINA : :
TEL/FAX:+86-535-6805690

B/L No: AHG0233130P08

Consignee
BRADLEYTHOMAS
ADDRESS : 25 MOLLER ST, GORDONVALE QLD 4865

AUSTRALIA C O P ¥

TEL:048 1042746

EMAIL:

BRADLEY@THOMASARBORICULTURE.COM.AU

Notify Party(Complete name and address) For delivery of goods Piease appl

SAME AS CONSIGNEE ASEA360 CONSOLIDATION PTY LTD

9/2 TULLAMARINE PARK ROAD, TULLAMARINE VIC 3043
PO BOX 659, TULLAMARINE VIC 3043

TEL: +613 8346 0166

FAX: +613 9338 7447

Place of Receipt

ANL GIPPSLAND 0828 Por SW ANGHALCHINA
BRISBANE AUSTRALIA Place BRISBANE, AUSTRALIA

PARTICULARS FURNISHED BY SHIPPER

Marks and numbers No. of pkgs. Description of goods Gross weight Measurement
1PKGS SAID TO CONTAIN 237KGS 0.92CBM
GRAPPLE
YT250305-07 HYDRAULIC ROTATOR

SEGU9412081/M1429568 40NOR

CFS-CFS
FREIGHT PREPAID
FREIGHT&CHARGE REVENUE TONS RATE COLLECT

OCEAN FREIGHT
SAY TOTAL:ONE PKGS ONLY

The goods and instructions are accepted and dealt with subject to the Standard Conditions printed overleaf

Taken in charge in apparent good order and condition, unless otherwise noted herein, at the place of receipt for transport and delivery as mentioned above.
One of these Combined Transport Bills of lading must be surrendered duly endorsed in exchange for the goods. In Witness whereof the original Combined
Transport Bills of Lading all of this tenor and date have been signed in the number stated below, one of which being accomplished the other(s) to be void.

Freight amount Place and date of issue
SHANGHAI MAY 04, 2025
Freight Payable at
SHANGHAI

Cargo Insurance through the undersigned
© not covered (1 Covered according to attached Policy
Number of Original


=== DOCUMENT SUMMARY ===
Total Pages: 1
Total Tables Found: 0
==================================================
Starting complete document extraction for: uploads/default/default_0db64855-78fd-4bc2-b93e-5a9df944713f.pdf
Processing page 1 of 1...
Performing comprehensive OCR on entire document...
Performing OCR on page 1...
=== DOCUMENT TEXT CONTENT (with OCR) ===
=== Page 1 ===
COMMERCIAL INVOICE
YANTAI RIMA MACHINERY CO., LTD.
Xibeiyu Industry Zone,Laizhou City,Shandong Province,China
TEL/FAX:+0086-535-6805690
TO: BRADLEYTHOMAS INVOICE NO.:YT250305-07
ADDRESS : 25 MOLLER ST, GORDONVALE QLD 4865
AUSTRALIA
TEL:0481042746
EMAIL: BRADLEY@THOMASARBORICULTURE.COM.AU
From : SHANGHAI,CHINA To:BRISBANE,AUSTRALIA Date:2025/3/5
Unit Price (USD)
Description & Packing Quantity (PCS) Amount (USD)
CFR BRISBANE
GRAPPLE 1 1100 1100
HYDRAULIC ROTATOR 1 278 278
CHARGE OF FREIGHT (BY SEA) 128
TTL: 1 PKG 1506

[OCR Text]
COMMERCIAL INVOICE

YANTAI RIMA MACHINERY CO., LTD.
Xibeiyu Industry Zone,Laizhou City,Shandong Province,China
TEL/FAX:+0086-535-6805690

TO: BRADLEYTHOMAS INVOICE NO.:¥T250305-07
ADDRESS : 25 MOLLER ST, GORDONVALE QLD 4865

AUSTRALIA

TEL:048 1042746

EMAIL: BRADLEY @THOMASARBORICULTURE.COM.AU

From : SHANGHALCHINA To: BRISBANE, AUSTRALIA Date:2025/3/5

ar : . Unit Price (USD)
Description & Packing Quantity (PCS) CER BRISBANE Amount (USD)

CHARGE OF FREIGHT (BY SEA

=== COMPREHENSIVE OCR EXTRACTION ===
=== OCR Page 1 ===
COMMERCIAL INVOICE

YANTAI RIMA MACHINERY CO., LTD.

Xibeiyu Industry Zone,Laizhou City,Shandong Province,China
TEL/FAX:+0086-535-6805690

TO: BRADLEYTHOMAS

ADDRESS : 25 MOLLER ST, GORDONVALE QLD 4865

AUSTRALIA
TEL:048 1042746

EMAIL: BRADLEY@THOMASARBORICULTURE.COM.AU

From : SHANGHAI,CHINA

To:BRISBANE,AUSTRALIA

INVOICE NO.:YT250305-07

Date:2025/3/5

Description & Packing Quantity (PCS) cre een Amount (USD)
GRAPPLE 1 1100 1100
HYDRAULIC ROTATOR 1 278 278
CHARGE OF FREIGHT (BY SEA) 128
TTL: 1 PKG 1506

=== EXTRACTED TABLES ===

=== Page_1_Table_1 ===
Dimensions: 4 rows × 4 columns

     Description & Packing Quantity (PCS) Unit Price (USD)\nCFR BRISBANE Amount (USD)
                   GRAPPLE              1                           1100         1100
         HYDRAULIC ROTATOR              1                            278          278
CHARGE OF FREIGHT (BY SEA)                                                        128
                      TTL:          1 PKG                                        1506
============================================================


=== DOCUMENT SUMMARY ===
Total Pages: 1
Total Tables Found: 1
==================================================

    """
    
#     # Create pipeline
#     pipeline = PreprocessingPipeline()
    
#     # Process text
#     result = pipeline.process(sample_text)

#     # llm._call()
    
#     # Convert to JSON
#     json_result = convert_result_to_json(result)
    
#     # Print JSON result
#     # print("=== PREPROCESSING PIPELINE RESULT (JSON) ===")
#     print(json.dumps(json_result, indent=2, ensure_ascii=False))
    
#     print(f"\n=== SUMMARY ===")
#     print(f"Original length: {result.original_length}")
#     print(f"Processed length: {result.processed_length}")
#     print(f"Reduction: {result.reduction_percentage:.1f}%")
#     print(f"Requires LLM: {result.requires_llm}")
#     print(f"Detected fields: {len(result.structured_data)}")
    
#     # Generate LLM prompt
#     llm_prompt = get_b650_section_a_extraction_prompt(sample_text)
#     prompt = llm_prompt.format(ocr_text=sample_text, declaration_type="import", structured_pipeline_data=result)
#     response = llm._call(prompt=prompt, response_format=B650_SECTION_A_RESPONSE_FORMAT)
#     print(f"llm_service LLM response: {response}")
#     print(f"LLM prompt length: {len(response)}")
    
#     if llm_prompt:
#         print(f"\n=== OPTIMIZED LLM PROMPT ===")
#         print(llm_prompt)

# if __name__ == "__main__":
#     example_usage()



## using it as a service

# Create pipeline
pipeline = PreprocessingPipeline()